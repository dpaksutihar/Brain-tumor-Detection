# -*- coding: utf-8 -*-
"""BraTS2023.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J-UeMFz6hxu9CeAcaXMROqlyCV2SGZDE
"""

import synapseclient
import synapseutils

syn = synapseclient.Synapse()
syn.login(authToken=authToken)
files = synapseutils.syncFromSynapse(syn, 'syn51156910')

print(f"Number of files downloaded: {len(files)}")
for file in files:
  print(file.path)

import os
import gzip
import shutil
import zipfile

# 1. Define the list of tumor types to organize
tumor_types = ['GLI', 'MEN', 'PED']

# 2. Define the base output directory
OUTPUT_DIR = 'data'

# Assuming 'files' is the list of synapseclient.File objects returned by syncFromSynapse
# files = [...]

def process_brats_files(files, tumor_types, output_base_dir):
    """
    Identifies file types and performs appropriate actions:
    ZIP extraction, NIfTI decompression, or simple copying,
    organizing by tumor type.
    """

    os.makedirs(output_base_dir, exist_ok=True)

    for file_obj in files:
        file_path = str(file_obj.path)
        file_name = os.path.basename(file_path)

        # 1. Filter out 'fastlane'
        if 'fastlane' in file_path.lower():
            continue

        # 2. Check for the tumor type
        for tumor_type in tumor_types:
            if tumor_type.lower() in file_path.lower():

                # Define the target directory for this tumor type
                target_dir = os.path.join(output_base_dir, tumor_type)
                os.makedirs(target_dir, exist_ok=True)

                print(f"\n--- Processing: **{file_name}** ---")

                try:
                    # ðŸš€ Handle ZIP Archives (.zip)
                    if file_path.lower().endswith('.zip'):
                        print(f"Extracting ZIP contents to: {target_dir}")
                        with zipfile.ZipFile(file_path, 'r') as zip_ref:
                            # Extract all contents into the specific tumor-type directory
                            zip_ref.extractall(target_dir)
                        print(f"âœ… Success: Extracted ZIP and saved to {target_dir}")

                    # ðŸ§  Handle NIfTI Decompression (.nii.gz)
                    elif file_path.lower().endswith('.nii.gz'):
                        output_file_path = os.path.join(target_dir, file_name[:-3]) # Remove '.gz'
                        print(f"Decompressing .nii.gz to {output_file_path}...")

                        with gzip.open(file_path, 'rb') as f_in:
                            with open(output_file_path, 'wb') as f_out:
                                shutil.copyfileobj(f_in, f_out)

                        print(f"âœ… Success: Decompressed NIfTI and saved to {target_dir}")

                    # ðŸ“‹ Handle other files (e.g., already uncompressed .nii, CSV, metadata)
                    else:
                        target_file_path = os.path.join(target_dir, file_name)
                        print(f"Copying file to {target_file_path}...")
                        shutil.copy2(file_path, target_file_path)
                        print(f"âœ… Success: Copied file to {target_dir}")

                    # Stop checking other tumor types for this file once a match is found
                    break

                except Exception as e:
                    print(f"âŒ Error processing {file_name}: {e}")
                    break

# Execute the function (assuming 'files' is your synchronized list)
process_brats_files(files, tumor_types, OUTPUT_DIR)

import glob
image_files = glob.glob('data/**/*.gz',recursive=True)
image_files[:5]

uncompressed_data = 'uncompressed_data'
import tqdm
for img_file in image_files:
    output_path = img_file.replace('data',uncompressed_data).replace('.gz','')
    os.makedirs(os.path.dirname(output_path) ,exist_ok=True)
    with gzip.open(img_file, 'rb') as f_in:
        with open(output_path, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)
    os.remove(img_file)

import os, glob, gzip, shutil
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

uncompressed_data = 'uncompressed_data'
data_path = 'images data'

# Create output directories
os.makedirs(os.path.join(data_path, 'images'), exist_ok=True)
os.makedirs(os.path.join(data_path, 'mask'), exist_ok=True)

# -------------------- Helper --------------------
def get_class_name(path):
    """
    Extracts the second directory name from a given path.
    Assumes a standard path structure with multiple directory levels.
    """
    # Normalize path to handle different OS separators and potential trailing slashes
    normalized_path = os.path.normpath(path)

    # Split the path into components
    path_components = normalized_path.split(os.sep)

    # Filter out empty strings that might result from leading/trailing slashes or multiple separators
    filtered_components = [comp for comp in path_components if comp]

    # Check if there are at least two directory components
    if len(filtered_components) >= 2:
        return filtered_components[1]
    else:
        return "None"


# ------------------ Process Files ----------------
gz_files = glob.glob('data/**/*.gz', recursive=True)

for gz_file in tqdm(gz_files):

    # Step 1: Unzip temporary .nii file
    nii_path = gz_file.replace('.gz', '').replace('data', uncompressed_data)
    os.makedirs(os.path.dirname(nii_path), exist_ok=True)

    with gzip.open(gz_file, 'rb') as f_in:
        with open(nii_path, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)

    # Step 2: Read nii
    nii_obj = nib.load(nii_path)
    data = nii_obj.get_fdata()
    slice_index = data.shape[2] // 2
    rotated_slice = np.rot90(data[:, :, slice_index], 3)

    # Step 3: Save output image/mask
    class_name = get_class_name(gz_file)
    os.makedirs(os.path.join(data_path, 'images', class_name), exist_ok=True)
    os.makedirs(os.path.join(data_path, 'mask', class_name), exist_ok=True)

    if 'seg.nii.gz' in gz_file:
        output_path = os.path.join(data_path, 'mask', class_name,
                                   os.path.basename(nii_path).replace('.nii', '.png'))
        rotated_slice = (rotated_slice > 0).astype(np.uint8)
    else:
        output_path = os.path.join(data_path, 'images', class_name,
                                   os.path.basename(nii_path).replace('.nii', '.png'))

    plt.imsave(output_path, rotated_slice, cmap='gray')

    # Step 4: Delete the extracted .nii to save storage
    os.remove(nii_path)

#!zip -r "images data.zip" "images data"

import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from matplotlib.gridspec import GridSpec
import warnings

# Suppress harmless Matplotlib warnings for cleaner output
warnings.filterwarnings("ignore", category=UserWarning)

# --- File Paths (same as before) ---
CASE_FOLDER = 'BraTS-GLI-00000-000'
FILE_PATHS = {
    'T1ce (Contrast)': '/content/BraTS-GLI-00000-000-t1c.nii',
    'T1n (Native)': '/content/BraTS-GLI-00000-000-t1n.nii',
    'T2-FLAIR': '/content/BraTS-GLI-00000-000-t2f.nii',
    'T2w': '/content/BraTS-GLI-00000-000-t2w.nii',
    'Segmentation (Mask)': '/content/BraTS-GLI-00000-000-seg.nii',
}

# --- 1. Load All NIfTI Files and Store Data ---
image_slices = {}
all_data = {}
all_shapes = []

try:
    for name, path in FILE_PATHS.items():
        if not os.path.exists(path):
            raise FileNotFoundError(f"File not found: {path}")

        nii_file = nib.load(path)
        data = nii_file.get_fdata()
        all_data[name] = data
        all_shapes.append(data.shape)

    slice_index = all_shapes[0][2] // 2
    print(f"Loading and extracting Axial Slice: {slice_index}")

    # --- 2. Extract, Rotate, and Store Slices ---
    for name, data in all_data.items():
        # Extract the 2D slice
        slice_2d = data[:, :, slice_index]

        # ROTATE 90 DEGREES RIGHT (k=3)
        rotated_slice = np.rot90(slice_2d, k=3)

        image_slices[name] = rotated_slice

    # Update mask data after rotation
    mask_slice = image_slices['Segmentation (Mask)']

    # --- 3. Prepare Mask for Visualization ---
    masked_data = np.ma.masked_where(mask_slice == 0, mask_slice)

    # ------------------------------------------------------------------
    # --- 4. Visualize All Slices with GridSpec for Custom Widths ---
    # ------------------------------------------------------------------

    fig = plt.figure(figsize=(22, 5))
    fig.suptitle(f"BraTS Case {CASE_FOLDER} - Rotated 90Â° Right (Axial Slice {slice_index})", fontsize=16)

    gs = GridSpec(1, 6, figure=fig, width_ratios=[1, 1, 1, 1, 1, 0.1])

    axes = []
    for i in range(5):
        axes.append(fig.add_subplot(gs[0, i]))

    ax_legend = fig.add_subplot(gs[0, 5])
    axes.append(ax_legend)

    modality_names = list(FILE_PATHS.keys())[:4]

    # 4. Plotting Loop (Images and Overlay)
    for i, name in enumerate(modality_names):
        ax = axes[i]
        ax.set_aspect('equal')

        ax.imshow(image_slices[name], cmap='gray')
        ax.imshow(masked_data, cmap='jet', alpha=0.5)
        ax.set_title(name, fontsize=10)
        ax.axis('off')

    # 5. Show the Ground Truth Mask (axes[4])
    ax_mask = axes[4]
    ax_mask.set_aspect('equal')

    im = ax_mask.imshow(mask_slice, cmap='viridis')
    ax_mask.set_title('Segmentation Mask', fontsize=10)
    ax_mask.axis('off')

    # 6. Add the Color Bar to the NEW, SMALL AXIS (ax_legend)
    cbar = fig.colorbar(im, cax=ax_legend)
    cbar.set_ticks([0, 1, 2, 4])
    cbar.set_ticklabels(['Background (0)', 'Necrotic (1)', 'Edema (2)', 'Enhancing (4)'])

    plt.tight_layout()
    plt.show()

except FileNotFoundError as e:
    print(f"\nERROR: File not found. Please ensure the path and file names are correct:\n{e}")
except Exception as e:
    print(f"\nAn error occurred during loading or visualization: {e}")

import os

def get_class_name(path):
    """
    Extracts the second directory name from a given path.
    Assumes a standard path structure with multiple directory levels.
    """
    # Normalize path to handle different OS separators and potential trailing slashes
    normalized_path = os.path.normpath(path)

    # Split the path into components
    path_components = normalized_path.split(os.sep)

    # Filter out empty strings that might result from leading/trailing slashes or multiple separators
    filtered_components = [comp for comp in path_components if comp]

    # Check if there are at least two directory components
    if len(filtered_components) >= 2:
        return filtered_components[1]
    else:
        return "None"

import nibabel as nib
import matplotlib.pyplot as plt
image_files = glob.glob(f'{uncompressed_data}/**/*.gz',recursive=True)
data_path = 'data'

for i in ['mask','images']:
  os.makedirs(os.path.join(data_path,i),exist_ok = True)

for input_path in image_files:
  class_name = get_class_name(input_path)
  output_path = os.path.join(data_path,'images',class_name,os.path.basename(input_path))
  nii_file = nib.load(input_path)
  data = nii_file.get_fdata()

  slice_index = data.shape[2] // 2
  slice_2d = data[:, :, slice_index]
  rotated_slice = np.rot90(slice_2d, k=3)

  if 'seg.nii' in input_path:
    output_path = os.path.join(data_path,'mask',class_name,os.path.basename(input_path))
    rotated_slice = (rotated_slice > 0).astype(np.uint8)

  plt.imsave(output_path, rotated_slice, cmap='gray')



"""# Segmentation"""

#!unzip "/content/drive/MyDrive/brain tumour segmentation/extracted data.zip" -d "/content"

import os
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras import layers, Model
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras.utils import Sequence
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import layers, Model
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import backend as K
from tensorflow.keras.applications import ResNet50

# Paths to dataset
IMAGE_DIR = "/content/extracted data/images"
MASK_DIR = "/content/extracted data/masks"
model_save_path = '/content/drive/MyDrive/brain tumour segmentation'
# Image dimensions & batch size
IMG_HEIGHT, IMG_WIDTH = 256, 256
BATCH_SIZE = 16
EPOCHS = 30

from tensorflow.keras import backend as K

def binary_crossentropy(y_true, y_pred, pos_weight=15):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())  # Avoid log(0)

    # Calculate the weighted binary cross-entropy
    loss = - (pos_weight * y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))
    return tf.reduce_mean(loss)
# Additional metrics
def iou_metric(y_true, y_pred, pos_weight=1):
    # Cast to float32
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)

    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection

    # Increase weight for positive class (1s)
    weighted_iou = intersection / (union + tf.keras.backend.epsilon()) * pos_weight
    return weighted_iou


def dice_coefficient(y_true, y_pred, pos_weight=1):
    # Cast to float32
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)

    intersection = tf.reduce_sum(y_true * y_pred)
    dice = (2 * intersection + tf.keras.backend.epsilon()) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + tf.keras.backend.epsilon())

    # Increase weight for positive class (1s)
    weighted_dice = dice * pos_weight
    return weighted_dice

import tensorflow as tf
from tensorflow.keras import backend as K

def dice_coefficient_multiclass(y_true, y_pred, num_classes=4):
    # Ensure y_true is correctly cast and one-hot encoded.
    # y_true shape: (B, H, W, 1) -> (B, H, W, num_classes)
    y_true_one_hot = K.one_hot(K.cast(y_true, 'int32'), num_classes=num_classes)

    # FIX: Remove the K.squeeze line entirely.
    # K.one_hot produced a 4D tensor (B, H, W, C), ready for flattening.

    y_pred_soft = y_pred

    # 1. Flatten the tensors to (B*H*W, C)

    # Flatten y_true: (B, H, W, C) -> (B*H*W*C)
    y_true_flat = K.flatten(y_true_one_hot)
    # Flatten y_pred: (B, H, W, C) -> (B*H*W*C)
    y_pred_flat = K.flatten(y_pred_soft)

    # 2. Reshape back to (B*H*W, num_classes)
    # The `K.flatten` followed by `K.reshape` into `(-1, num_classes)`
    # is the correct way to merge the spatial dimensions (B, H, W)
    # while keeping the class dimension (C) separated.
    y_true_flat = K.reshape(y_true_flat, (-1, num_classes))
    y_pred_flat = K.reshape(y_pred_flat, (-1, num_classes))

    # 3. Calculate Dice for each class
    # Intersection = (y_true * y_pred) summed over all pixels for each class (axis=0)
    intersection = K.sum(y_true_flat * y_pred_flat, axis=0)

    # Sum of areas = (y_true + y_pred) summed over all pixels for each class (axis=0)
    sum_of_areas = K.sum(y_true_flat, axis=0) + K.sum(y_pred_flat, axis=0)

    # Dice = (2 * Intersection) / (Sum of Areas)
    dice = (2. * intersection + K.epsilon()) / (sum_of_areas + K.epsilon())

    # 4. Calculate the mean Dice across all classes
    # If you want to exclude background (class 0), use K.mean(dice[1:])
    return K.mean(dice)

# U-Net Model# U-Net Model with ResNet Encoder
def unet_resnet_model(input_size=(IMG_HEIGHT, IMG_WIDTH, 3)):
    # Load the pre-trained ResNet50 model, excluding the top layers (classification layers)
    resnet = ResNet50(include_top=False, weights='imagenet', input_shape=input_size)

    # Encoder (ResNet50)
    c1 = resnet.get_layer('conv1_relu').output  # First layer from ResNet
    c2 = resnet.get_layer('conv2_block3_out').output  # Intermediate layer
    c3 = resnet.get_layer('conv3_block4_out').output  # Intermediate layer
    c4 = resnet.get_layer('conv4_block6_out').output  # Intermediate layer

    # Bottleneck
    c5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)
    c5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c5)

    # Decoder (Upsampling)
    u1 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)
    u1 = layers.concatenate([u1, c3])
    c6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u1)
    c6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c6)

    u2 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)
    u2 = layers.concatenate([u2, c2])
    c7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u2)
    c7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c7)

    u3 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)
    u3 = layers.concatenate([u3, c1])
    c8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u3)
    c8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c8)

    # Final upsampling layer to match target shape (256x256)
    u4 = layers.Conv2DTranspose(1, (2, 2), strides=(2, 2), padding='same')(c8)  # Final output (256x256)
    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(u4)

    model = Model(inputs=resnet.input, outputs=outputs)
    return model

# Function to get all image/mask paths
def get_image_paths(image_dir, mask_dir, class_names):
    image_paths, mask_paths = [], []
    for class_name in class_names:
        img_folder = os.path.join(image_dir, class_name)
        mask_folder = os.path.join(mask_dir, class_name)

        for filename in os.listdir(img_folder):
            image_paths.append(os.path.join(img_folder, filename))
            # mask_paths.append(os.path.join(mask_folder, filename))

    return image_paths


# Data Generator
class ImageMaskGenerator(Sequence):
    def __init__(self, image_paths, batch_size, img_size=(IMG_HEIGHT, IMG_WIDTH), shuffle=True,
                 class_names = [],
                 **kwargs):
        super().__init__(**kwargs)
        self.image_paths = image_paths
        # self.mask_paths = mask_paths
        self.batch_size = batch_size
        self.img_size = img_size
        self.shuffle = shuffle
        self.class_names = class_names
        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.image_paths) / self.batch_size))

    def __getitem__(self, index):
        batch_image_paths = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]
        # batch_mask_paths = self.mask_paths[index * self.batch_size:(index + 1) * self.batch_size]

        images, masks = self.__load_batch(batch_image_paths)
        return np.array(images), np.array(masks).astype(np.uint8)

    def __load_batch(self, image_paths):
        images, masks = [], []
        for img_path in image_paths:
            image = cv2.imread(img_path, cv2.IMREAD_COLOR)
            image = cv2.resize(image, self.img_size)

            mask_path = img_path.replace('images','mask').replace('t2w','seg').replace('t2f','seg').replace('t1n','seg').replace('t1c','seg')


            # i = 0
            # for i,name in enumerate(self.class_names):
            #   if name in mask_path:
            #     i=i+1
            #     break
            if os.path.exists(mask_path):
              mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
            else:
              mask = np.zeros(self.img_size, dtype=np.uint8)
            mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)

            # mask = np.zeros((256, 256, 4), dtype=np.uint8)
            # mask[:, :, i] = (ori_mask > 0).astype(np.uint8)
            # ori_mask = ori_mask*i

            # mask = np.expand_dims(mask, axis=-1)
            # msak = np.repeat(np.zeros_like(ori_mask),4,axis=2)
            # mask[:,:,i] = ori_mask

            mask = mask / 255.0  # Normalize to [0,1]

            rows, cols, _ = image.shape
            max_rotation_angle = 10
            # Generate a random angle between -max_rotation_angle and +max_rotation_angle
            angle = np.random.uniform(-max_rotation_angle, max_rotation_angle)
            # Get the rotation matrix (M) centered at the image center
            M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)
            # Apply rotation to the image (using INTER_LINEAR for smooth interpolation)
            image = cv2.warpAffine(image, M, (cols, rows), borderMode=cv2.BORDER_CONSTANT)
            # Apply rotation to the mask (using INTER_NEAREST to preserve discrete class labels)
            mask = cv2.warpAffine(mask, M, (cols, rows), borderMode=cv2.BORDER_CONSTANT,
                                    flags=cv2.INTER_NEAREST) # <--- **CRITICAL CHANGE**

            image = image / 255.0  # Normalize
            mask = np.expand_dims(mask, axis=-1)  # Add channel dimension
            images.append(image)
            masks.append(mask)

        return images, masks

    def on_epoch_end(self):
        if self.shuffle:
            # combined = list(zip(self.image_paths, self.mask_paths))
            np.random.shuffle(self.image_paths)
            # self.image_paths, self.mask_paths = zip(*combined)

# Get class names (folders in images/masks directory)
class_names = sorted(os.listdir(IMAGE_DIR))
print('class_names:',class_names)
# Load image & mask file paths
image_paths = get_image_paths(IMAGE_DIR, MASK_DIR, class_names)

# Split into train (70%), val (20%), test (10%)
train_imgs, temp_imgs = train_test_split(image_paths, test_size=0.1, random_state=42)
val_imgs, test_imgs = train_test_split(temp_imgs, test_size=1/3, random_state=42)

print(f"Train: {len(train_imgs)}, Val: {len(val_imgs)}, Test:{len(test_imgs)}")

# Create generators
train_gen = ImageMaskGenerator(train_imgs, BATCH_SIZE,class_names = class_names, shuffle=True,)
val_gen = ImageMaskGenerator(val_imgs, BATCH_SIZE,class_names = class_names, shuffle=False,)
X, y = next(iter(val_gen))

print("Image Batch Shape (X.shape):", X.shape)
print("Mask Batch Shape (y.shape):", y.shape)

y.max()

import numpy as np
import matplotlib.pyplot as plt

print(f"Total images in batch: {X.shape[0]}")

# --- 2. Define Display Parameters ---
num_samples_to_show = 4
num_classes = 4 # Adjust this if your actual class count is different

# --- 3. Get Random Indices ---
# Generate random indices to pick images from the batch
batch_size = X.shape[0]
random_indices = np.random.choice(batch_size, size=num_samples_to_show, replace=False)

# --- 4. Plot the Samples ---
fig, axes = plt.subplots(num_samples_to_show, 2, figsize=(8, 4 * num_samples_to_show))

# Create a colormap for the 4 classes (0-3)
# Class 0 (Background) is typically black/dark.
cmap = plt.cm.get_cmap('viridis', num_classes)

fig.suptitle(f"Random Samples (Batch Size: {batch_size})", fontsize=14, y=1.02)

for i, index in enumerate(random_indices):
    # Get image and mask
    image = X[index]
    # Squeeze the mask to remove the last dimension (H, W, 1) -> (H, W)
    mask = np.squeeze(y[index])  # (H, W, C)

    # # Sum pixels per channel â†’ detect non-empty class channels
    # channel_sums = mask.sum(axis=(0, 1))

    # # Index(es) of channels containing mask
    # active_channels = np.where(channel_sums > 0)[0]
    # if len(active_channels) > 0:
    #     # Pick the first active class mask
    #     selected_channel = active_channels[0]
    #     class_mask = mask[:, :, selected_channel]
    # else:
    #     # No class present â†’ background
    #     class_mask = np.zeros(mask.shape[:2], dtype=np.uint8)


    # Plot Image
    axes[i, 0].imshow(image)
    axes[i, 0].set_title(f"Sample {index} - Input Image")
    axes[i, 0].axis('off')

    # Plot Mask (using the custom colormap)
    # Note: If your mask contains values > 3, the colormap will need adjustment.
    mask_plot = axes[i, 1].imshow(mask, cmap=cmap, vmin=0, vmax=num_classes - 1)
    axes[i, 1].set_title(f"Sample {index} - Multi-Class Mask")
    axes[i, 1].axis('off')


plt.tight_layout(rect=[0, 0, 0.9, 1.0])
plt.show()

# # Compile model with additional metrics

def binary_crossentropy(y_true, y_pred, pos_weight=15):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())  # Avoid log(0)

    # Calculate the weighted binary cross-entropy
    loss = - (pos_weight * y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))
    return tf.reduce_mean(loss)
# Additional metrics
def iou_metric(y_true, y_pred, pos_weight=1):
    # Cast to float32
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)

    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection

    # Increase weight for positive class (1s)
    weighted_iou = intersection / (union + tf.keras.backend.epsilon()) * pos_weight
    return weighted_iou


def dice_coefficient(y_true, y_pred, pos_weight=1):
    # Cast to float32
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)

    intersection = tf.reduce_sum(y_true * y_pred)
    dice = (2 * intersection + tf.keras.backend.epsilon()) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + tf.keras.backend.epsilon())

    # Increase weight for positive class (1s)
    weighted_dice = dice * pos_weight
    return weighted_dice

model = unet_resnet_model()
model.compile(optimizer='adam',
              loss=binary_crossentropy,
              metrics=['accuracy', iou_metric, dice_coefficient])

# print(model.summary())
# Callbacks
checkpoint = ModelCheckpoint(os.path.join(model_save_path,"best_resnet_unet_segmentation_model.keras"),
                             save_best_only=True,
                             save_weights_only=False,
                             monitor="val_accuracy",
                             mode = 'max',
                             verbose=1)

early_stop = EarlyStopping(monitor="val_accuracy",
                           patience=5,
                           mode = 'max',
                           restore_best_weights=True,
                           verbose=1)

reduce_lr = ReduceLROnPlateau(monitor="val_accuracy",
                              factor=0.1,
                              patience=3,
                              mode = 'max',
                              min_lr=1e-6,
                              verbose=1)
# Train model using generator
history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    callbacks=[checkpoint, early_stop, reduce_lr],
)

# Save final trained model
# model.save("unet_segmentation_model_last.keras")
print("Model training complete and saved!")

model = unet_resnet_model()

import tensorflow as tf
from tensorflow.keras import backend as K

def dice_coefficient(y_true, y_pred):
    y_pred = tf.cast(y_pred > 0.5, tf.float32)  # Thresholding for metric only
    y_true = tf.cast(y_true, tf.float32)

    intersection = tf.reduce_sum(y_true * y_pred)
    return (2 * intersection + 1e-7) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1e-7)

def dice_loss(y_true, y_pred):
    return 1 - dice_coefficient(y_true, y_pred)

def mean_iou_score(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred > 0.5, tf.float32)  # Binarize here

    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection

    return (intersection + 1e-7) / (union + 1e-7)

loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)

model.compile(
    optimizer='adam',
    loss=loss_fn,
    metrics=[
        'accuracy',
        mean_iou_score,
        dice_coefficient
    ]
)

checkpoint = ModelCheckpoint(os.path.join(model_save_path,"best_resnet_unet_segmentation_model.keras"),
                             save_best_only=True,
                             save_weights_only=False,
                             monitor="val_accuracy",
                             mode = 'max',
                             verbose=1)

early_stop = EarlyStopping(monitor="val_accuracy",
                           patience=5,
                           mode = 'max',
                           restore_best_weights=True,
                           verbose=1)

reduce_lr = ReduceLROnPlateau(monitor="val_accuracy",
                              factor=0.1,
                              patience=3,
                              mode = 'max',
                              min_lr=1e-6,
                              verbose=1)
# Train model using generator
history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    callbacks=[checkpoint, early_stop, reduce_lr],
)

# Save final trained model
# model.save("unet_segmentation_model_last.keras")
print("Model training complete and saved!")

# Plot training history
plt.figure(figsize=(12, 4))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label="Train Accuracy")
plt.plot(history.history['val_accuracy'], label="Val Accuracy")
plt.legend()
plt.title("Accuracy")

# Loss Plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label="Train Loss")
plt.plot(history.history['val_loss'], label="Val Loss")
plt.legend()
plt.title("Loss")

plt.show()

# Optionally, you can plot IoU and Dice
plt.figure(figsize=(12, 4))

# IoU Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['mean_iou_score'], label="Train IoU")
plt.plot(history.history['val_mean_iou_score'], label="Val IoU")
plt.legend()
plt.title("IoU Metric")

# Dice Coefficient Plot
plt.subplot(1, 2, 2)
plt.plot(history.history['dice_coefficient'], label="Train Dice")
plt.plot(history.history['val_dice_coefficient'], label="Val Dice")
plt.legend()
plt.title("Dice Coefficient")

plt.show()

# Evaluate on test set
test_gen = ImageMaskGenerator(test_imgs, BATCH_SIZE, shuffle=False, class_names= class_names)
test_loss, test_acc, test_iou, test_dice = model.evaluate(test_gen)
print(f"Test Accuracy: {test_acc:.2f}")
print(f"Test IoU: {test_iou:.2f}")
print(f"Test Dice Coefficient: {test_dice:.2f}")

model.save(os.path.join(model_save_path,"BrainSegmentaion.keras"))

"""# classification model"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model
import cv2
import numpy as np

IMG_SIZE = (256, 256)
BATCH = 16
NUM_CLASSES = 3

train_dir = "/content/extracted data/images"
val_dir = "/content/extracted data/images"

# ---------------- Preprocessing Function ---------------- #
def custom_preprocess(img):
    # Convert to grayscale (image comes as uint8 [0,255])
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

    # # CLAHE for enhancing contrast
    # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(5, 5))
    # gray = clahe.apply(gray)

    # # Gaussian Blurring for noise reduction
    # gray = cv2.GaussianBlur(gray, (3, 3), 0)

    # # Expand channel dimension (224,224,1)
    # gray = gray[..., np.newaxis]

    # Normalize â†’ [0,1]
    gray = gray / 255.0

    return gray

VAL_SPLIT = 0.2

# ---------------- Data Augmentation ---------------- #
train_gen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    rescale=1./255,   # <-- normalize to [0,1]
    horizontal_flip=True,
    fill_mode="nearest",
    # preprocessing_function=custom_preprocess,
    validation_split=VAL_SPLIT   # <-- IMPORTANT
)

val_gen = ImageDataGenerator(
    rescale=1./255,   # <-- normalize to [0,1]
    # preprocessing_function=custom_preprocess,
    validation_split=VAL_SPLIT
)

# ---------------- Create train subset ---------------- #
train_data = train_gen.flow_from_directory(
    train_dir,                  # path to single folder with all classes
    target_size=IMG_SIZE,
    color_mode='grayscale',
    batch_size=BATCH,
    class_mode='categorical',
    shuffle=True,
    subset='training'           # <-- take training subset
)

# ---------------- Create validation subset ---------------- #
val_data = val_gen.flow_from_directory(
    train_dir,                  # same folder as above
    target_size=IMG_SIZE,
    color_mode='grayscale',
    batch_size=BATCH,
    class_mode='categorical',
    shuffle=False,
    subset='validation'         # <-- take validation subset
)

# ---------------- Model Builder ---------------- #
def build_model(pretrained=True):
    input_tensor = Input(shape=(256, 256, 1))

    # Convert grayscale -> pseudo RGB for DenseNet
    x = tf.keras.layers.Concatenate()([input_tensor, input_tensor, input_tensor])

    base = tf.keras.applications.DenseNet121(
        include_top=False,
        weights="imagenet" if pretrained else None,
        input_tensor=x
    )

    base.trainable = True  # Fine-tuning allowed

    x = Conv2D(128, (3, 3), activation='relu')(base.output)
    x = Conv2D(128, (3, 3), activation='relu')(x)
    x = Flatten()(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(3, activation='softmax')(x)

    return Model(inputs=input_tensor, outputs=outputs)
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau



# ---------------- Train Function ---------------- #
def train(model, name):

      # ---------------- Callbacks ---------------- #
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        ModelCheckpoint(
            filepath=f'/content/drive/MyDrive/brain tumour segmentation/{name}.h5',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=3,
            min_lr=1e-7,
            verbose=1
        )
    ]
    model.compile(
        optimizer=tf.keras.optimizers.Adam(1e-4),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )

    history = model.fit(
        train_data,
        validation_data=val_data,
        epochs=30,
        callbacks=callbacks,   # <--- Added here
        verbose=1
    )

    model.save(f"{name}.h5")
    print(f"Saved {name}.h5")
    return history


# ================= PLOT TRAINING HISTORY ================= #
def plot_history(history, title):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(14,5))

    # Accuracy Plot
    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, label='Train Accuracy')
    plt.plot(epochs, val_acc, label='Validation Accuracy')
    plt.title(f'{title} - Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss Plot
    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, label='Train Loss')
    plt.plot(epochs, val_loss, label='Validation Loss')
    plt.title(f'{title} - Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


# ================= CLASSIFICATION REPORT ================= #
def evaluate_model(model, data, name):
    y_true = data.classes
    class_labels = list(data.class_indices.keys())

    # Predictions
    pred = model.predict(data)
    y_pred = np.argmax(pred, axis=1)

    print(f"\n========= {name} Classification Report =========")
    print(classification_report(y_true, y_pred, target_names=class_labels))

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d',
                xticklabels=class_labels,
                yticklabels=class_labels,
                cmap='Blues')
    plt.title(f'{name} - Confusion Matrix')
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# ------- Train #1: Pretrained DenseNet121 ------- #
model_pretrained = build_model(pretrained=True)
history_pretrained = train(model_pretrained, "DenseNet_Clf_pretrained")

import matplotlib.pyplot as plt
plot_history(history_pretrained, "DenseNet Pretrained")

# ------- Train #2: Non-Pretrained DenseNet121 ------- #
model_scratch = build_model(pretrained=False)
history_scratch = train(model_scratch, "DenseNet_Clf_scratch")

plot_history(history_scratch, "DenseNet Scratch")

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import numpy as np

evaluate_model(model_pretrained, val_data, "DenseNet Pretrained")

# Plot both models
# Evaluate both models
evaluate_model(model_scratch, val_data, "DenseNet Scratch")

